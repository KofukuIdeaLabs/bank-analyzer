{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vuV2AR83dOqz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cqjan98DdVqp"
      },
      "outputs": [],
      "source": [
        "class BankLogoDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all bank folders\n",
        "        self.banks = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "        self.class_to_idx = {bank: idx for idx, bank in enumerate(sorted(self.banks))}\n",
        "\n",
        "        # Initialize lists to store paths and labels\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Collect paths and labels\n",
        "        for bank in self.banks:\n",
        "            bank_dir = self.root_dir / bank\n",
        "            if not bank_dir.exists():\n",
        "                print(f\"Warning: {bank_dir} does not exist\")\n",
        "                continue\n",
        "\n",
        "            for img_path in bank_dir.glob('*'):\n",
        "                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tif']:\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.labels.append(self.class_to_idx[bank])\n",
        "\n",
        "        # Print dataset statistics\n",
        "        self._print_stats()\n",
        "\n",
        "    def _print_stats(self):\n",
        "        label_counts = Counter(self.labels)\n",
        "        print(\"\\nDataset Statistics:\")\n",
        "        print(\"------------------\")\n",
        "        for bank, idx in self.class_to_idx.items():\n",
        "            count = label_counts[idx]\n",
        "            print(f\"{bank}: {count} images\")\n",
        "        print(f\"Total: {len(self.labels)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            Image.MAX_IMAGE_PIXELS = None\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            return torch.zeros((3, 224, 224)), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7zBSad6bdXPl"
      },
      "outputs": [],
      "source": [
        "class BankClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BankClassifier, self).__init__()\n",
        "        # Load pre-trained ResNet50\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Freeze early layers\n",
        "        for param in list(self.model.parameters())[:-4]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Modify the final layer for our number of classes\n",
        "        num_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Sequential(\n",
        "            nn.Linear(num_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wI8kIwzCdZ5m"
      },
      "outputs": [],
      "source": [
        "def get_transforms(train=True):\n",
        "    \"\"\"Get transforms with heavy augmentation for training\"\"\"\n",
        "    if train:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(20),\n",
        "            transforms.RandomAffine(\n",
        "                degrees=0,\n",
        "                translate=(0.1, 0.1),  # Random translation\n",
        "                scale=(0.9, 1.1),      # Random scaling\n",
        "                shear=10               # Random shearing\n",
        "            ),\n",
        "            transforms.ColorJitter(\n",
        "                brightness=0.3,\n",
        "                contrast=0.3,\n",
        "                saturation=0.2,\n",
        "                hue=0.1\n",
        "            ),\n",
        "            transforms.RandomGrayscale(p=0.1),  # Randomly convert to grayscale\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j3HAxtszdb4P"
      },
      "outputs": [],
      "source": [
        "def create_balanced_sampler(dataset):\n",
        "    \"\"\"Create a weighted sampler to balance the dataset\"\"\"\n",
        "    class_counts = Counter(dataset.labels)\n",
        "    weights = [1.0 / class_counts[label] for label in dataset.labels]\n",
        "    weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=weights,\n",
        "        num_samples=len(dataset) * 2,  # Oversample to 2x the original dataset size\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    return sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qyqgJcKAdfKW"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20):\n",
        "    \"\"\"Train the model with validation and early stopping\"\"\"\n",
        "    best_val_acc = 0.0\n",
        "    patience = 5  # Number of epochs to wait for improvement\n",
        "    patience_counter = 0\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Save best model and check for early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "            }, 'best_bank_classifier.pth')\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3vLXNPGTdg1W"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
        "    \"\"\"Plot training and validation losses and accuracies\"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss') \n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBLYnjvZdiwB",
        "outputId": "946895b3-f7a2-4b39-ba7c-03eb06c3b9d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Dataset Statistics:\n",
            "------------------\n",
            "AU Small Finance Bank: 4 images\n",
            "Axis Bank: 14 images\n",
            "Bandhan Bank: 2 images\n",
            "Bank of Baroda: 13 images\n",
            "Bank of India: 2 images\n",
            "Bank of Maharashtra: 2 images\n",
            "CSB Bank: 2 images\n",
            "Canara Bank: 4 images\n",
            "Central Bank of India: 4 images\n",
            "City Union Bank: 2 images\n",
            "DCB Bank: 2 images\n",
            "Dhanlaxmi Bank: 2 images\n",
            "ESFB: 2 images\n",
            "Federal Bank: 26 images\n",
            "HDFC Bank: 40 images\n",
            "ICICI Bank: 43 images\n",
            "IDBI Bank: 8 images\n",
            "IDFC Bank: 17 images\n",
            "India Post Payments Bank: 6 images\n",
            "Indian Bank: 6 images\n",
            "Indian Overseas Bank: 2 images\n",
            "Induslnd Bank: 11 images\n",
            "J&K bank: 2 images\n",
            "Karnataka Bank: 2 images\n",
            "Karur Vysya Bank: 2 images\n",
            "Kerala Gramin Bank: 2 images\n",
            "Kotak Bank: 10 images\n",
            "Paytm: 2 images\n",
            "Punjab & Sindh Bank: 2 images\n",
            "Punjab National Bank: 57 images\n",
            "RBL Bank: 9 images\n",
            "SBI: 24 images\n",
            "South India Bank: 2 images\n",
            "Tamil Nad Mercentile Bank: 2 images\n",
            "The Nainital Bank: 2 images\n",
            "UCO Bank: 2 images\n",
            "Union Bank of India: 4 images\n",
            "Yes Bank: 23 images\n",
            "Total: 361 images\n",
            "\n",
            "Dataset Statistics:\n",
            "------------------\n",
            "AU Small Finance Bank: 4 images\n",
            "Axis Bank: 14 images\n",
            "Bandhan Bank: 2 images\n",
            "Bank of Baroda: 13 images\n",
            "Bank of India: 2 images\n",
            "Bank of Maharashtra: 2 images\n",
            "CSB Bank: 2 images\n",
            "Canara Bank: 4 images\n",
            "Central Bank of India: 4 images\n",
            "City Union Bank: 2 images\n",
            "DCB Bank: 2 images\n",
            "Dhanlaxmi Bank: 2 images\n",
            "ESFB: 2 images\n",
            "Federal Bank: 26 images\n",
            "HDFC Bank: 40 images\n",
            "ICICI Bank: 43 images\n",
            "IDBI Bank: 8 images\n",
            "IDFC Bank: 17 images\n",
            "India Post Payments Bank: 6 images\n",
            "Indian Bank: 6 images\n",
            "Indian Overseas Bank: 2 images\n",
            "Induslnd Bank: 11 images\n",
            "J&K bank: 2 images\n",
            "Karnataka Bank: 2 images\n",
            "Karur Vysya Bank: 2 images\n",
            "Kerala Gramin Bank: 2 images\n",
            "Kotak Bank: 10 images\n",
            "Paytm: 2 images\n",
            "Punjab & Sindh Bank: 2 images\n",
            "Punjab National Bank: 57 images\n",
            "RBL Bank: 9 images\n",
            "SBI: 24 images\n",
            "South India Bank: 2 images\n",
            "Tamil Nad Mercentile Bank: 2 images\n",
            "The Nainital Bank: 2 images\n",
            "UCO Bank: 2 images\n",
            "Union Bank of India: 4 images\n",
            "Yes Bank: 23 images\n",
            "Total: 361 images\n",
            "\n",
            "Dataset Statistics:\n",
            "------------------\n",
            "AU Small Finance Bank: 4 images\n",
            "Axis Bank: 14 images\n",
            "Bandhan Bank: 2 images\n",
            "Bank of Baroda: 13 images\n",
            "Bank of India: 2 images\n",
            "Bank of Maharashtra: 2 images\n",
            "CSB Bank: 2 images\n",
            "Canara Bank: 4 images\n",
            "Central Bank of India: 4 images\n",
            "City Union Bank: 2 images\n",
            "DCB Bank: 2 images\n",
            "Dhanlaxmi Bank: 2 images\n",
            "ESFB: 2 images\n",
            "Federal Bank: 26 images\n",
            "HDFC Bank: 40 images\n",
            "ICICI Bank: 43 images\n",
            "IDBI Bank: 8 images\n",
            "IDFC Bank: 17 images\n",
            "India Post Payments Bank: 6 images\n",
            "Indian Bank: 6 images\n",
            "Indian Overseas Bank: 2 images\n",
            "Induslnd Bank: 11 images\n",
            "J&K bank: 2 images\n",
            "Karnataka Bank: 2 images\n",
            "Karur Vysya Bank: 2 images\n",
            "Kerala Gramin Bank: 2 images\n",
            "Kotak Bank: 10 images\n",
            "Paytm: 2 images\n",
            "Punjab & Sindh Bank: 2 images\n",
            "Punjab National Bank: 57 images\n",
            "RBL Bank: 9 images\n",
            "SBI: 24 images\n",
            "South India Bank: 2 images\n",
            "Tamil Nad Mercentile Bank: 2 images\n",
            "The Nainital Bank: 2 images\n",
            "UCO Bank: 2 images\n",
            "Union Bank of India: 4 images\n",
            "Yes Bank: 23 images\n",
            "Total: 361 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20:\n",
            "Train Loss: 3.631 | Train Acc: 5.68%\n",
            "Val Loss: 3.458 | Val Acc: 2.49%\n",
            "Epoch 2/20:\n",
            "Train Loss: 3.306 | Train Acc: 15.65%\n",
            "Val Loss: 3.365 | Val Acc: 6.09%\n",
            "Epoch 3/20:\n",
            "Train Loss: 3.010 | Train Acc: 23.96%\n",
            "Val Loss: 3.159 | Val Acc: 14.13%\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Dataset paths\n",
        "    data_dir = \"/content/drive/MyDrive/logos\"  # Update this path\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = BankLogoDataset(data_dir, transform=None)\n",
        "\n",
        "    # Split indices for train/val\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(full_dataset)),\n",
        "        test_size=0.2,\n",
        "        stratify=full_dataset.labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create train and validation datasets\n",
        "    train_dataset = BankLogoDataset(data_dir, transform=get_transforms(train=True))\n",
        "    val_dataset = BankLogoDataset(data_dir, transform=get_transforms(train=False))\n",
        "\n",
        "    # Create balanced sampler for training\n",
        "    train_sampler = create_balanced_sampler(train_dataset)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    num_classes = len(full_dataset.banks)\n",
        "    model = BankClassifier(num_classes).to(device)\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "    # Train model\n",
        "    train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        device,\n",
        "        num_epochs=20\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    print(f\"Model saved as 'best_bank_classifier.pth'\")\n",
        "    print(f\"Training plot saved as 'training_history.png'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
