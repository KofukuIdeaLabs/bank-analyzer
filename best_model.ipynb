{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10424066,"sourceType":"datasetVersion","datasetId":6461044}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"vuV2AR83dOqz","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T04:29:23.990843Z","iopub.execute_input":"2025-01-13T04:29:23.991336Z","iopub.status.idle":"2025-01-13T04:29:23.996502Z","shell.execute_reply.started":"2025-01-13T04:29:23.991298Z","shell.execute_reply":"2025-01-13T04:29:23.995479Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class BankLogoDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n\n        # Get all bank folders\n        self.banks = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n        self.class_to_idx = {bank: idx for idx, bank in enumerate(sorted(self.banks))}\n\n        # Initialize lists to store paths and labels\n        self.image_paths = []\n        self.labels = []\n\n        # Collect paths and labels\n        for bank in self.banks:\n            bank_dir = self.root_dir / bank\n            if not bank_dir.exists():\n                print(f\"Warning: {bank_dir} does not exist\")\n                continue\n\n            for img_path in bank_dir.glob('*'):\n                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tif']:\n                    self.image_paths.append(img_path)\n                    self.labels.append(self.class_to_idx[bank])\n\n        # Print dataset statistics\n        # self._print_stats()\n\n    def _print_stats(self):\n        label_counts = Counter(self.labels)\n        print(\"\\nDataset Statistics:\")\n        print(\"------------------\")\n        for bank, idx in self.class_to_idx.items():\n            count = label_counts[idx]\n            print(f\"{bank}: {count} images\")\n        print(f\"Total: {len(self.labels)} images\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n\n        try:\n            Image.MAX_IMAGE_PIXELS = None\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image, label\n        except Exception as e:\n            print(f\"Error loading {img_path}: {e}\")\n            return torch.zeros((3, 224, 224)), label","metadata":{"id":"cqjan98DdVqp","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T04:29:23.998332Z","iopub.execute_input":"2025-01-13T04:29:23.998987Z","iopub.status.idle":"2025-01-13T04:29:24.013223Z","shell.execute_reply.started":"2025-01-13T04:29:23.998947Z","shell.execute_reply":"2025-01-13T04:29:24.012459Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class BankClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(BankClassifier, self).__init__()\n        # Load pre-trained ResNet50\n        self.model = models.efficientnet_v2_l(pretrained=True)\n\n        # Freeze early layers\n        for param in list(self.model.parameters())[:-4]:\n            param.requires_grad = False\n\n        # Modify the final layer for our number of classes\n        num_features = self.model.classifier[1].in_features\n        self.model.classifier[1] = nn.Sequential(\n            nn.Linear(num_features, 512),  # Increase width\n            nn.ReLU(),\n            nn.BatchNorm1d(512),          # Add batch normalization\n            nn.Dropout(0.3),              # Try different dropout rates\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"id":"7zBSad6bdXPl","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T04:43:38.631541Z","iopub.execute_input":"2025-01-13T04:43:38.632336Z","iopub.status.idle":"2025-01-13T04:43:38.638250Z","shell.execute_reply.started":"2025-01-13T04:43:38.632291Z","shell.execute_reply":"2025-01-13T04:43:38.637407Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def get_transforms(train=True):\n    \"\"\"Get transforms with heavy augmentation for training\"\"\"\n    if train:\n        return transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(20),\n            transforms.RandomAffine(\n                degrees=0,\n                translate=(0.1, 0.1),  # Random translation\n                scale=(0.9, 1.1),      # Random scaling\n                shear=10               # Random shearing\n            ),\n            transforms.ColorJitter(\n                brightness=0.3,\n                contrast=0.3,\n                saturation=0.2,\n                hue=0.1\n            ),\n            transforms.RandomGrayscale(p=0.1),  # Randomly convert to grayscale\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n    else:\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])","metadata":{"id":"wI8kIwzCdZ5m","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T04:29:24.025601Z","iopub.execute_input":"2025-01-13T04:29:24.025895Z","iopub.status.idle":"2025-01-13T04:29:24.035789Z","shell.execute_reply.started":"2025-01-13T04:29:24.025869Z","shell.execute_reply":"2025-01-13T04:29:24.034968Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_balanced_sampler(dataset):\n    \"\"\"Create a weighted sampler to balance the dataset\"\"\"\n    class_counts = Counter(dataset.labels)\n    weights = [1.0 / class_counts[label] for label in dataset.labels]\n    weights = torch.DoubleTensor(weights)\n\n    sampler = WeightedRandomSampler(\n        weights=weights,\n        num_samples=len(dataset) * 2,  # Oversample to 2x the original dataset size\n        replacement=True\n    )\n\n    return sampler","metadata":{"id":"j3HAxtszdb4P","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T04:29:24.037278Z","iopub.execute_input":"2025-01-13T04:29:24.037531Z","iopub.status.idle":"2025-01-13T04:29:24.047402Z","shell.execute_reply.started":"2025-01-13T04:29:24.037491Z","shell.execute_reply":"2025-01-13T04:29:24.046762Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20):\n    \"\"\"Train the model with validation and early stopping\"\"\"\n    best_val_acc = 0.0\n    patience = 5  # Number of epochs to wait for improvement\n    patience_counter = 0\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    # Initially freeze all layers except the final layer\n    for param in model.model.features.parameters():  # Note the double model (one is the class, one is ResNet)\n        param.requires_grad = False\n    \n    # Keep final layers trainable\n    for param in model.model.classifier[1].parameters():\n        param.requires_grad = True\n\n    for epoch in range(num_epochs):\n        if epoch == 5:  # Unfreeze last sequential block\n            print(\"Unfreezing last sequential block...\")\n            for param in model.model.features[-1].parameters():\n                param.requires_grad = True\n            optimizer = optim.Adam([\n                {'params': model.model.classifier.parameters(), 'lr': 0.001},\n                {'params': model.model.features[-1].parameters(), 'lr': 0.0001}\n            ], weight_decay=0.01)\n                \n        elif epoch == 10:  # Unfreeze second to last sequential block\n            print(\"Unfreezing second to last sequential block...\")\n            for param in model.model.features[-2].parameters():\n                param.requires_grad = True\n            optimizer = optim.Adam([\n                {'params': model.model.classifier.parameters(), 'lr': 0.001},\n                {'params': model.model.features[-1].parameters(), 'lr': 0.0001},\n                {'params': model.model.features[-2].parameters(), 'lr': 0.00001}\n            ], weight_decay=0.01)\n            \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        train_correct = 0\n        train_total = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            train_total += labels.size(0)\n            train_correct += predicted.eq(labels).sum().item()\n\n        train_acc = 100. * train_correct / train_total\n        train_loss = running_loss / len(train_loader)\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n\n        val_acc = 100. * val_correct / val_total\n        val_loss = val_loss / len(val_loader)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.2f}%')\n\n        # Save best model and check for early stopping\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_val_acc': best_val_acc,\n            }, 'best_bank_classifier.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        '''if patience_counter >= patience:\n            print(f'Early stopping triggered after {epoch + 1} epochs')\n            break'''\n\n    return train_losses, val_losses, train_accs, val_accs","metadata":{"id":"qyqgJcKAdfKW","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T05:49:38.459742Z","iopub.execute_input":"2025-01-13T05:49:38.460451Z","iopub.status.idle":"2025-01-13T05:49:38.473630Z","shell.execute_reply.started":"2025-01-13T05:49:38.460409Z","shell.execute_reply":"2025-01-13T05:49:38.472746Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n    \"\"\"Plot training and validation losses and accuracies\"\"\"\n    plt.figure(figsize=(12, 5))\n    \n    # Plot losses\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss') \n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Plot accuracies\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label='Training Accuracy')\n    plt.plot(val_accs, label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"id":"3vLXNPGTdg1W","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T04:35:01.046280Z","iopub.execute_input":"2025-01-13T04:35:01.046637Z","iopub.status.idle":"2025-01-13T04:35:01.053223Z","shell.execute_reply.started":"2025-01-13T04:35:01.046606Z","shell.execute_reply":"2025-01-13T04:35:01.052229Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def main():\n    # Set random seed for reproducibility\n    torch.manual_seed(42)\n\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Dataset paths\n    data_dir = \"/kaggle/input/bank-logos/logos\"  # Update this path\n\n    # Create full dataset\n    full_dataset = BankLogoDataset(data_dir, transform=None)\n\n    # Split indices for train/val\n    train_indices, val_indices = train_test_split(\n        range(len(full_dataset)),\n        test_size=0.2,\n        stratify=full_dataset.labels,\n        random_state=42\n    )\n\n    # Create train and validation datasets\n    train_dataset = BankLogoDataset(data_dir, transform=get_transforms(train=True))\n    val_dataset = BankLogoDataset(data_dir, transform=get_transforms(train=False))\n\n    # Create balanced sampler for training\n    train_sampler = create_balanced_sampler(train_dataset)\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=128,\n        sampler=train_sampler,\n        num_workers=4,\n        pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=128,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n\n    # Initialize model\n    num_classes = len(full_dataset.banks)\n    print(num_classes)\n    model = BankClassifier(num_classes).to(device)\n\n    # Define loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.05)\n\n    # Train model\n    train_losses, val_losses, train_accs, val_accs = train_model(\n        model,\n        train_loader,\n        val_loader,\n        criterion,\n        optimizer,\n        device,\n        num_epochs=50\n    )\n\n    # Plot training history\n    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n\n    print(\"Training completed!\")\n    print(f\"Model saved as 'best_bank_classifier.pth'\")\n\nif __name__ == '__main__':\n    main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBLYnjvZdiwB","outputId":"946895b3-f7a2-4b39-ba7c-03eb06c3b9d9","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T06:51:09.772223Z","iopub.execute_input":"2025-01-13T06:51:09.772918Z","iopub.status.idle":"2025-01-13T06:51:12.963910Z","shell.execute_reply.started":"2025-01-13T06:51:09.772883Z","shell.execute_reply":"2025-01-13T06:51:12.962330Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n37\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_L_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_L_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_bank_classifier.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[43], line 57\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m train_losses, val_losses, train_accs, val_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     65\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[1;32m     68\u001b[0m plot_training_history(train_losses, val_losses, train_accs, val_accs)\n","Cell \u001b[0;32mIn[31], line 43\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     41\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     44\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_image(model_path, image_path, device):\n    # First load the dataset to get the correct number of classes\n    dataset = BankLogoDataset(\"/kaggle/input/bank-logos/logos\")\n    num_classes = len(dataset.banks)  # This should be 37 based on your error\n    # Create model with correct number of classes\n    model = BankClassifier(num_classes=num_classes)\n    \n    # Load the state dict\n    checkpoint = torch.load(model_path, weights_only=True)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n\n    # Transform for inference\n    transform = get_transforms(train=False)\n    \n    # Load and transform image\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n    \n    # Get prediction\n    with torch.no_grad():\n        output = model(image)\n        prob = torch.nn.functional.softmax(output, dim=1)\n        pred_class = output.argmax(1).item()\n        confidence = prob[0][pred_class].item()\n    \n    # Get class name from the same dataset instance\n    pred_bank = {v: k for k, v in dataset.class_to_idx.items()}[pred_class]\n    \n    return pred_bank, confidence\n\n# Usage example:\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbank, conf = predict_image('/kaggle/working/best_bank_classifier.pth', '/kaggle/input/bank-logos/logos/Axis Bank/ea9455f8-1e54-4b77-b547-a19938110c22_first_page.png', device)\nprint(f'Predicted bank: {bank} with confidence: {conf:.2%}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T06:58:51.109868Z","iopub.execute_input":"2025-01-13T06:58:51.110712Z","iopub.status.idle":"2025-01-13T06:58:54.533902Z","shell.execute_reply.started":"2025-01-13T06:58:51.110661Z","shell.execute_reply":"2025-01-13T06:58:54.532882Z"}},"outputs":[{"name":"stdout","text":"Predicted bank: Axis Bank with confidence: 62.44%\n","output_type":"stream"}],"execution_count":50}]}